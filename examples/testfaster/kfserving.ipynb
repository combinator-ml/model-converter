{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "democratic-princess",
   "metadata": {},
   "source": [
    "# KFServing deploying demo\n",
    "\n",
    "## Deploy Chassis built image to KFServing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broadband-receptor",
   "metadata": {},
   "source": [
    "This demo will show you how we can deploy an image built using `Chassis` to an existing installation of KFServing.\n",
    "\n",
    "This is assuming that you have already containerizer your model using `Chassis` service and the image can be downloaded from a public Docker Hub account."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "treated-response",
   "metadata": {},
   "source": [
    "## Prepare KFServing deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compliant-luther",
   "metadata": {},
   "source": [
    "First we define the image tag that has been pushed to Docker Hub.\n",
    "\n",
    "Example: `carmilso/chassisml-sklearn-demo:latest`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "verified-miller",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image tag: carmilso/chassisml-sklearn-demo:latest\n"
     ]
    }
   ],
   "source": [
    "image_tag = input('Image tag: ')\n",
    "\n",
    "import os\n",
    "os.environ['IMAGE_TAG'] = image_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "senior-image",
   "metadata": {},
   "source": [
    "Now we create the KFServing deployment definition. We will use the image built with Chassis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "passive-documentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cat <<- EOF > kfserving_v1.yaml\n",
    "apiVersion: \"serving.kubeflow.org/v1beta1\"\n",
    "kind: \"InferenceService\"\n",
    "metadata:\n",
    "  name: \"chassis-kfserving-v1\"\n",
    "spec:\n",
    "  predictor:\n",
    "    containers:\n",
    "    - image: \"$IMAGE_TAG\"\n",
    "      name: \"chassis-kfserving-v1-container\"\n",
    "      env:\n",
    "        - name: INTERFACE\n",
    "          value: kfserving\n",
    "        - name: HTTP_PORT\n",
    "          value: \"8080\"\n",
    "        - name: PROTOCOL\n",
    "          value: v1\n",
    "        - name: MODEL_NAME\n",
    "          value: digits\n",
    "      ports:\n",
    "        - containerPort: 8080\n",
    "          protocol: TCP\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-delhi",
   "metadata": {},
   "source": [
    "## Deploy the image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sublime-minnesota",
   "metadata": {},
   "source": [
    "Now, we are going to apply that configuration file so our image is deployed in the KFServing installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete-street",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all these commands on testfaster SSH client.\n",
    "kubectl cp $(kubectl get pod -l app=combinator-jupyter -o custom-columns=\":metadata.name\" | tr -d '\\n'):kfserving_v1.yaml kfserving_v1.yaml\n",
    "kubectl apply -f kfserving_v1.yaml\n",
    "\n",
    "export INGRESS_HOST=$(minikube ip)\n",
    "export INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name==\"http2\")].nodePort}')\n",
    "export SERVICE_NAME=chassis-kfserving-v1\n",
    "export MODEL_NAME=digits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba59925",
   "metadata": {},
   "source": [
    "Once we have defined the required variables we need to wait for the pod to be up and running. Once this is finished we can get the service hostname."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabd2d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all these commands on testfaster SSH client.\n",
    "# Wait until the pod is created and run the following command.\n",
    "export SERVICE_HOSTNAME=$(kubectl get inferenceservice ${SERVICE_NAME} -o jsonpath='{.status.url}' | cut -d \"/\" -f 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ff211b",
   "metadata": {},
   "source": [
    "We copy the inputs file that contains the data that we want to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a73ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all these commands on testfaster SSH client.\n",
    "kubectl cp $(kubectl get pod -l app=combinator-jupyter -o custom-columns=\":metadata.name\" | tr -d '\\n'):containerizer/service/flavours/mlflow/interfaces/kfserving/inputsv1.json inputsv1.json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71589f5d",
   "metadata": {},
   "source": [
    "## Predict data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82692006",
   "metadata": {},
   "source": [
    "Now you can just make a request to predict some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2349ab12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all these commands on testfaster SSH client.\n",
    "curl -H \"Host: ${SERVICE_HOSTNAME}\" \"http://${INGRESS_HOST}:${INGRESS_PORT}/v1/models/${MODEL_NAME}:predict\" -d@inputsv1.json | jq\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
